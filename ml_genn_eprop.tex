%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
\usepackage{balance} % For balanced columns on the last page
\usepackage[binary-units]{siunitx}

% Visible TODO notes
\newcommand{\todo}[1]{\textbf{\textsc{\textcolor{red}{(TODO: #1)}}}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2022}
\acmYear{2022}
\setcopyright{rightsretained}
\acmConference[NICE 2022]{Neuro-Inspired Computational Elements Conference}{March 28-April 1, 2022}{Virtual Event, USA}
\acmBooktitle{Neuro-Inspired Computational Elements Conference (NICE 2022), March 28-April 1, 2022, Virtual Event, USA}
\acmDOI{10.1145/3517343.3517346}
\acmISBN{978-1-4503-9559-5/22/03}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Efficient GPU training of LSNNs using eProp}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{James C Knight}
\email{J.C.Knight@sussex.ac.uk}
\orcid{0000-0003-0577-0074}
\affiliation{%
  \institution{University of Sussex}
  \department{School of Engineering and Informatics}
  \city{Brighton}
  \country{United Kingdom}
}

\author{Thomas Nowotny}
\email{T.Nowotny@sussex.ac.uk}
\orcid{0000-0002-4451-915X}
\affiliation{%
  \institution{University of Sussex}
  \department{School of Engineering and Informatics}
  \city{Brighton}
  \country{United Kingdom}
}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Knight and Nowotny}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
    To demonstrate the value of mlGeNN in this space, we present the results of an exploration of shallow classifier architectures for the classification of the DVS gesture dataset. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257.10010293.10011809</concept_id>
       <concept_desc>Computing methodologies~Bio-inspired approaches</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010258.10010259</concept_id>
       <concept_desc>Computing methodologies~Supervised learning</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010169.10010170.10010173</concept_id>
       <concept_desc>Computing methodologies~Vector / streaming algorithms</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Bio-inspired approaches}
\ccsdesc[300]{Computing methodologies~Supervised learning}
\ccsdesc[300]{Computing methodologies~Vector / streaming algorithms}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{spiking neural networks, efficient simulation, GPU}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The development of efficient SNN simulators has been a key area of computational neuroscience research for several decades~\citep{carnevale2006neuron, Gewaltig2007, Golosio2021, Akar2019,Yavuz2016}.
However, these simulators are not well-suited to the types of model and the workflows required for spike-based machine learning research.
As such, many ML researchers have chosen to build libraries~\citep{norse2021, SpikingJelly, eshraghian2021training,Hazan2018} on top of more familiar tools such as PyTorch.
However, while libraries like PyTorch are highly-optimised for rate-based models, they does not take advantage of the spatio-temporal sparsity of SNNs which have the potential to enable massive computational savings over rate-based networks~\citep{Yin2021}.
Specialised neuromorphic hardware~\citep{Davies2018,Furber2014,Merolla2014} can be used to efficiently implement SNNs but, while some consensus is beginning to emerge on the theoretical framework within which biological learning may operate, casting algorithms into hardware always results in a game of catch-up with even the most flexible current neuromorphic systems being capable of implementing only very limited versions of the latest online learning rules~\todo{cite charlotte, melika}. 

While previous libraries for spike-based ML~\todo{cite spiking jelly, bindsnet, norse, neko} have allowed SNNs to be defined in a familiar environment for ML researchers, they have been implemented on top of libraries like PyTorch.
When using these libraries, the activity of a population of neurons is typically represented as a vector of rates and, for an SNN, this vector is populated with ones for spiking and zeros for non-spiking neurons. 
This representation allows one to apply the existing infrastructure of the underlying ML library to SNNs but, as real neurons often spike at comparatively low rates, propagating the activity of inactive neurons through the network leads to unnecessary computation.
Additionally, the connections between populations of real neurons are sparse and continuously ‘rewired’ during learning23 which, using a standard ML library, would typically be implemented as a weight matrix containing many zeros.
Both choices are inefficient so, we have developed a new library for spike-based ML which harnesses  GPU-optimised sparse data structures and algorithms provided by our GeNN simulator~\todo{cite ourselves!}.

We previously presented the first version of mlGeNN~\todo{cite} which provided workflows for converting ANNs trained using TensorFlow~\todo{cite} into SNNs for simulation using GeNN.
However, as we found, 

\section{mlGeNN}
\subsection{Model description}

\subsection{Model compilation}

\section{Results}
In order to demonstrate the usefulness of mlGeNN for spike-based ML research, here we present the results of a small exploration of SNN architectures, trained using e-prop
\subsection{Accuracy}
\subsection{Performance}
\begin{itemize}
    \item Training time - compare to FPTT
\end{itemize}

\section{Conclusions}
While the results we have presented in this paper demonstrate that the approximations behind the eProp learning rule do not necessarily prevent it enablig competitive performane in relatively complex datasets, eProp does have some significant issues.
\begin{itemize}
    \item Eligibility traces are unique to each pair of connected pre and postsynaptic neurons so cannot be efficiently added to convolutional models
\end{itemize}
However, the eProp learning rule requires time-driven updates which dominate the time taken to \emph{train} these models.
Therefore, we are working to implement the fully event-driven EventProp~\citep{Wunderlich2021} learning rule in GeNN which will allow training times to also benefit from temporal sparsity.
Finally, the models presented in this paper are all densely connected so are not taking advantage of connection sparsity.
We are working in parallel to address this by combining these learning rules with the Deep-R~\citep{Bellec2018a} rewiring rule, enabling SNN classifiers to take advantage of GeNN's support for efficient sparse connectivity~\citep{Knight2018}.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This work was funded by the EPSRC (grant numbers EP/V052241/1 and EP/S030964/1) and the EU's Horizon 2020 program (grant agreement 945539).
Compute time was provided through Gauss Centre for Supercomputing application number 21018 and EPSRC (grant number EP/T022205/1).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{eprop}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
